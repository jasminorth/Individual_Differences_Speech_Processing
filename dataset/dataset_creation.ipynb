{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f628971c",
   "metadata": {},
   "source": [
    "# Dataset Creation Notebook\n",
    "\n",
    "\n",
    "\n",
    "explain what i do (plus links to datasets)\n",
    "\n",
    "\n",
    "In this Notebook, we read from three datasets in total: (Links are provided in the README part of the dataset folder)\n",
    "* `BRM-emot-submit.csv`: This dataset gives us our stimuli and their emotional valence $\\to$ `stimuli_and_valence_df`\n",
    "* `SUBTLEXusfrequencyabove1.xls`: This dataset gives us frequencies of words $\\to$ `freq_df`\n",
    "* `CogNet-v1.0.tsv`: this dataset gives us words and their cognates from any language $\\to$ `cognate_df`\n",
    "\n",
    "We will take our potential stimuli from the `stimuli_and_valence_df` and by checking against pre-set conditions, only retain the ones that meet them.\n",
    "The resulting stimuli (words) are then categorized into positive, negative, and neutral, depending on their valence values.\n",
    "\n",
    "> Note: we are parsing pretty big datasets here, so running this might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25776426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positiv: 142, Negativ: 179, Neutral: 595, None: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Item_type</th>\n",
       "      <th>Stimulus</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Cognate_status</th>\n",
       "      <th>Emotional_valence_cat</th>\n",
       "      <th>Emotional_valence_cont</th>\n",
       "      <th>Length</th>\n",
       "      <th>Lg10SUBTLEX_US</th>\n",
       "      <th>correct_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ABDOMEN</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.43</td>\n",
       "      <td>7</td>\n",
       "      <td>2.235528</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ABILITY</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.991669</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ABSORB</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.50</td>\n",
       "      <td>6</td>\n",
       "      <td>2.008600</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ABUSE</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.53</td>\n",
       "      <td>5</td>\n",
       "      <td>2.719331</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ACCOUNT</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.39</td>\n",
       "      <td>7</td>\n",
       "      <td>3.358125</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>YEN</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.495544</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>YUMMY</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.52</td>\n",
       "      <td>5</td>\n",
       "      <td>2.359835</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ZAP</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3</td>\n",
       "      <td>2.100371</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ZIP</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.06</td>\n",
       "      <td>3</td>\n",
       "      <td>2.591065</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>LDT</td>\n",
       "      <td>exp</td>\n",
       "      <td>ZIPPER</td>\n",
       "      <td>word</td>\n",
       "      <td>noncognate</td>\n",
       "      <td>neutral</td>\n",
       "      <td>5.11</td>\n",
       "      <td>6</td>\n",
       "      <td>2.161368</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Experiment Item_type Stimulus Condition Cognate_status  \\\n",
       "0          LDT       exp  ABDOMEN      word     noncognate   \n",
       "1          LDT       exp  ABILITY      word     noncognate   \n",
       "2          LDT       exp   ABSORB      word     noncognate   \n",
       "3          LDT       exp    ABUSE      word     noncognate   \n",
       "4          LDT       exp  ACCOUNT      word     noncognate   \n",
       "..         ...       ...      ...       ...            ...   \n",
       "911        LDT       exp      YEN      word     noncognate   \n",
       "912        LDT       exp    YUMMY      word     noncognate   \n",
       "913        LDT       exp      ZAP      word     noncognate   \n",
       "914        LDT       exp      ZIP      word     noncognate   \n",
       "915        LDT       exp   ZIPPER      word     noncognate   \n",
       "\n",
       "    Emotional_valence_cat  Emotional_valence_cont  Length  Lg10SUBTLEX_US  \\\n",
       "0                 neutral                    5.43       7        2.235528   \n",
       "1                positive                    7.00       7        2.991669   \n",
       "2                 neutral                    5.50       6        2.008600   \n",
       "3                negative                    1.53       5        2.719331   \n",
       "4                 neutral                    5.39       7        3.358125   \n",
       "..                    ...                     ...     ...             ...   \n",
       "911               neutral                    5.00       3        2.495544   \n",
       "912              positive                    7.52       5        2.359835   \n",
       "913               neutral                    5.39       3        2.100371   \n",
       "914               neutral                    5.06       3        2.591065   \n",
       "915               neutral                    5.11       6        2.161368   \n",
       "\n",
       "    correct_response  \n",
       "0                  j  \n",
       "1                  j  \n",
       "2                  j  \n",
       "3                  j  \n",
       "4                  j  \n",
       "..               ...  \n",
       "911                j  \n",
       "912                j  \n",
       "913                j  \n",
       "914                j  \n",
       "915                j  \n",
       "\n",
       "[916 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: comments & make pretty\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# -----------------------------\n",
    "# Load datasets\n",
    "# -----------------------------\n",
    "\"\"\" we read the datasets that we have collected (i.e. downloaded)\n",
    "-> if you run this, make sure they are in the same folder as this code notebook\"\"\"\n",
    "stimuli_and_valence_df = pd.read_csv(\"BRM-emot-submit.csv\", index_col=0)        # from this dataset we get the words themselves, along with their emotional valence\n",
    "freq_df = pd.read_excel(\"SUBTLEXusfrequencyabove1.xls\")        # from this dataset we get the frequency of these words\n",
    "cognate_df = pd.read_csv(\"CogNet-v1.0.tsv\", sep=\"\\t\")        # from this dataset we get the cognate status\n",
    "\n",
    "# -----------------------------\n",
    "# Precompute frequency dictionary\n",
    "# -----------------------------\n",
    "\"\"\"we parse our frequency dataset and take only the \"Lg10WF\" frequencies from it\n",
    "-> because the dataset is very big and this helps with computation time\"\"\"\n",
    "freq_dict = dict(zip(freq_df[\"Word\"].astype(str), freq_df[\"Lg10WF\"])) # e.g., {\"dog\": 2.111, ...}\n",
    "\n",
    "# -----------------------------\n",
    "# Precompute POS tags for ALL stimuli\n",
    "# -----------------------------\n",
    "\"\"\"there is the so-called \"nltk-pos_tag\" - this gives us part-of-speech tags (Wortarten)\n",
    "for all of our potential words (sounds weird to do it for all words if we only use a few of them later, but its actually quicker somehow)\n",
    "-> sometimes its not too great at assigning POS tags, so we still have to go over your data manually later to find potential mistakes\n",
    "-> but its the easiest and quickest way to do this, especially for nouns (like in our case)\"\"\"\n",
    "stimuli = stimuli_and_valence_df[\"Word\"].astype(str).tolist()\n",
    "tagged = dict(nltk.pos_tag(stimuli))   # e.g., {\"dog\": \"NN\", ...}\n",
    "\n",
    "# -----------------------------\n",
    "# Precompute cognate lookup\n",
    "# -----------------------------\n",
    "\"\"\"we read the cognate dataset. cognates in this dataset are shown as word1, language1 and the corresponding word2, language2 (in one row).\n",
    "To make things easier for us, we only take the rows of the dataset where either language1 or language2 is english and the other one is german\"\"\"\n",
    "# Create a set of English words where the matching row\n",
    "# contains German (\"deu\") as the other language.\n",
    "cognate_set = set()\n",
    "\n",
    "for _, r in cognate_df.iterrows():\n",
    "    w1, l1 = r[\"word 1\"], r[\"lang 1\"]\n",
    "    w2, l2 = r[\"word 2\"], r[\"lang 2\"]\n",
    "\n",
    "    # We only care about words where one language is english and the *other* language is German                     ###\n",
    "    if l1 == \"eng\" and l2 == \"deu\":\n",
    "        cognate_set.add(str(w1))\n",
    "    if l2 == \"eng\" and l1 == \"deu\":\n",
    "        cognate_set.add(str(w2))\n",
    "\n",
    "# -----------------------------\n",
    "# Process rows\n",
    "# -----------------------------\n",
    "\"\"\"Now that we have prepared everything that we need, we can start processing our dataset.\n",
    "This means that for each word, we check our dataset conditions (through the \"if\").\n",
    "Dataset conditions:\n",
    "- length shorter than 8 characters (i.e. maximum 7 characters long)\n",
    "- part-of-speech is noun\n",
    "- frequency higher than 2\n",
    "- not a cognate\n",
    "\n",
    "If any of these conditions are not met, we \"continue\", which means that the word will dropped (i.e. not be added to the dataset)\n",
    "and we continue with the next one.\n",
    "\n",
    "If the word passed all the checks, we look up its emotional valence and assign a categorical value to it (pos/neg/neutr)\n",
    "- pos: continuuos valence of 7 or higher\n",
    "- neg: continuuos valence of 3 or lower\n",
    "- neutral: continuuos valence between (or equal to) 4.5 and 5.5\n",
    "everything that falls outside of that range will be left out of the dataset, because we want there to be be clear distinctions between the valences\n",
    "-> we dont want fuzzy boundaries\n",
    "\n",
    "After doing all that, we will add the word to our dataset, along with a lot more info that we have collected about our words along the way\n",
    "\n",
    "Note: we will count how many of each valence we have: while we want to have 30 of each in the end, ideally, at this point, this number should be higher though,\n",
    "as we want to go over the dataset manually later (to account for any potentail mistakes with pos and non-cognate assignments).\n",
    "we also want to make sure that we only have clear-valence-words in there, so we will check that manually too. \n",
    "Therefore, the numbers should be high enough to account for any manual post-hoc deletions.\n",
    "\"\"\"\n",
    "rows = []\n",
    "neg_count = pos_count = neutr_count = none_count = 0\n",
    "\n",
    "for _, row in stimuli_and_valence_df.iterrows():\n",
    "    stimulus = str(row[\"Word\"])\n",
    "    valence = row[\"V.Mean.Sum\"]\n",
    "\n",
    "    # Length filter\n",
    "    if len(stimulus) > 7:\n",
    "        continue\n",
    "\n",
    "    # POS filter (noun)\n",
    "    tag = tagged.get(stimulus, \"\")\n",
    "    if not tag.startswith(\"NN\"):\n",
    "        continue\n",
    "\n",
    "    # Frequency filter\n",
    "    freq = freq_dict.get(stimulus, 0) # if the word does not exist in the freq dataset, we assume their freq is 0\n",
    "    if freq <= 2:\n",
    "        continue\n",
    "\n",
    "    # Cognate check\n",
    "    ## for now, we will assume that anythng that is not in the cognate dataset is actually not a cognate\n",
    "    ## of course this is probably not the case, as the word could just not be included in the dataset, even though it is one\n",
    "    ## we will go over this and check it manually later\n",
    "    if stimulus in cognate_set:\n",
    "        continue\n",
    "\n",
    "    # process valence into categorical (cat_valence) and continuuos (valence)\n",
    "    cat_valence = None\n",
    "\n",
    "\n",
    "    if valence <= 3:\n",
    "        cat_valence = \"negative\"\n",
    "        neg_count += 1\n",
    "\n",
    "    elif valence >= 7:\n",
    "        cat_valence = \"positive\"\n",
    "        pos_count += 1\n",
    "\n",
    "    elif 4.5 <= valence <= 5.5:\n",
    "        cat_valence = \"neutral\"\n",
    "        neutr_count += 1\n",
    "\n",
    "    else:\n",
    "        valence = None\n",
    "        continue            # skip fuzzy boundary cases\n",
    "\n",
    "    # add the word, along with its continuuos and categoircal valence, its length, and frequency\n",
    "    if pd.notna(valence):\n",
    "        rows.append({\n",
    "            \"Experiment\": \"LDT\",\n",
    "            \"Item_type\": \"exp\",\n",
    "            \"Stimulus\": stimulus.upper(),\n",
    "            \"Condition\": \"word\",\n",
    "            \"Cognate_status\": \"noncognate\",\n",
    "            \"Emotional_valence_cat\": cat_valence,###\n",
    "            \"Emotional_valence_cont\": valence,###\n",
    "            \"Length\": len(stimulus),\n",
    "            \"Lg10SUBTLEX_US\": freq,\n",
    "            \"correct_response\": \"j\"\n",
    "        })\n",
    "\n",
    "# -----------------------------\n",
    "# Output\n",
    "# -----------------------------\n",
    "print(f\"Positiv: {pos_count}, Negativ: {neg_count}, Neutral: {neutr_count}, None: {none_count}\")\n",
    "exp_df = pd.DataFrame(rows)\n",
    "exp_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2822fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "exp_df.to_csv(\"dataframe_exp.csv\", encoding='utf-8', index=False, sep = \";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
